{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ed4a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"WANDB_MODE\"]=\"disabled\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "#!export CUDA_VISIBLE_DEVICES=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d63c2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer,BertForSequenceClassification,Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn import preprocessing\n",
    "import torch.nn as nn\n",
    "from sklearn.utils import compute_class_weight\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import pickle\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10698e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "# use this to map categories to integers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64ced250",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #load tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', do_lower_case=True, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "925724d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfaff96",
   "metadata": {},
   "source": [
    "functions and class definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba50c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassDataset(Dataset):\n",
    "\n",
    "    def __init__(self, encodings, labels):\n",
    "      self.encodings = encodings\n",
    "      self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54c732a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(dataframe, tokenizer):\n",
    "  MAX_LENGTH = 100 #raise from 60 to 80 to 100\n",
    "  inputs = {\n",
    "          \"input_ids\":[],\n",
    "          \"attention_mask\":[]\n",
    "        }\n",
    "   \n",
    "\n",
    "  # use this if concatenting text from all fields in the data (except the class field)\n",
    "  features_columns =[x for x in dataframe.columns.values if x != 'iSampleMaterial']\n",
    "  def create_concatenated_text(dataframe):\n",
    "    \"\"\"combine the columns text to create a single sentence\"\"\"\n",
    "    ttext= [] #text that is a concatenation of all columns\n",
    "    for _, row in dataframe.iterrows():\n",
    "      row_value = row[\"text\"]\n",
    "      if row_value!=\"\" and type(row_value)==str:\n",
    "          combined += row_value   # +\" , \"\n",
    "      ttext.append(combined)\n",
    "    return ttext\n",
    "  #sents = create_concatenated_text(dataframe)\n",
    "    \n",
    "\n",
    "  def getTrainText(dataframe, ttcol ):  # smr version-- pre concatenate training text in one column\n",
    "    ttext= []\n",
    "    for _, row in dataframe.iterrows():\n",
    "      row_value = row[ttcol]\n",
    "      ttext.append(str(row_value))\n",
    "    return ttext\n",
    "\n",
    "  sents = getTrainText(dataframe, traintextcol )\n",
    "    \n",
    "  for sent in sents:\n",
    "    tokenized_input = tokenizer(sent,max_length=MAX_LENGTH, padding='max_length', truncation=True)\n",
    "    inputs[\"input_ids\"].append(torch.tensor(tokenized_input[\"input_ids\"]))\n",
    "    inputs[\"attention_mask\"].append(torch.tensor(tokenized_input[\"attention_mask\"]))\n",
    " \n",
    "  print(\"torch tensor dataframe columns:\", dataframe.columns.values)\n",
    "  #print(\"dataframe['iSampleMaterial']: \",dataframe['iSampleMaterial'].values )\n",
    "  labels = torch.tensor(dataframe['iSampleMaterial'].values.tolist())\n",
    "    \n",
    "  return MulticlassDataset(inputs,labels)\n",
    "\n",
    "def get_class_weights(dataframe):\n",
    "  \"\"\"computes the class weight and returns a list to account for class imbalance \"\"\"\n",
    "  \n",
    "    \n",
    "  dataframe['iSampleMaterial'] = le.transform(dataframe.iSampleMaterial) \n",
    "  labels = torch.tensor(dataframe['iSampleMaterial'].values.tolist())\n",
    "  #labels = torch.tensor(dataframe['iSampleMaterial'].values.tolist())\n",
    "  label_le = le.classes_ \n",
    "  print (\"np unique labels for weights:\", np.unique(labels))  \n",
    "  print (\"le class labels: \",label_le)\n",
    "  print (\"labels.numpy:\", labels.numpy)\n",
    "    \n",
    "  class_weights=compute_class_weight( class_weight ='balanced',classes = np.unique(labels),y = labels.numpy())\n",
    "  #class_weights=compute_class_weight( class_weight ='balanced',classes = labels,y = labellist.numpy())\n",
    "\n",
    "  total_class_weights =torch.tensor(class_weights,dtype=torch.float).to(device)\n",
    "  return total_class_weights\n",
    "\n",
    "def create_custom_trainer(class_weights):\n",
    "  \"\"\"creates custom trainer that accounts for class imbalance\"\"\"\n",
    "  class CustomTrainer(Trainer):\n",
    "      def compute_loss(self, model, inputs, return_outputs=False):\n",
    "          labels = inputs.get(\"labels\")\n",
    "          # forward pass\n",
    "          outputs = model(**inputs)\n",
    "          logits = outputs.get(\"logits\")\n",
    "          # compute custom loss \n",
    "          loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
    "          loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "          return (loss, outputs) if return_outputs else loss\n",
    "  return CustomTrainer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb24080",
   "metadata": {},
   "source": [
    "def train(selected_type, dataframe, tokenizer, batch_size, learning_rate, epochs,train_mode, output_dir):\n",
    "\n",
    "  train_df, dev_df, test_df = preprocess(dataframe,selected_type)\n",
    "  train_dataset = create_dataset(train_df, tokenizer)\n",
    "  dev_dataset = create_dataset(dev_df,tokenizer)\n",
    "  test_dataset = create_dataset(test_df,tokenizer)\n",
    "\n",
    "  #load model\n",
    "  model = BertForSequenceClassification.from_pretrained(\"allenai/scibert_scivocab_uncased\", num_labels = len(le.classes_), )\n",
    "\n",
    "  # Tell pytorch to run this model on the GPU.\n",
    "  #desc = model.cuda()\n",
    "  desc = model.to(device)\n",
    "\n",
    "  training_args = TrainingArguments(\n",
    "          output_dir= output_dir,     # output directory\n",
    "          num_train_epochs=epochs,              # total number of training epochs\n",
    "          per_device_train_batch_size=batch_size,  # batch size per device during training\n",
    "          per_device_eval_batch_size=batch_size,   # batch size for evaluation\n",
    "          learning_rate = learning_rate,\n",
    "          warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "          weight_decay=0.01, \n",
    "          load_best_model_at_end=True,            \n",
    "          logging_dir=output_dir,            # directory for storing logs\n",
    "          logging_steps=10,\n",
    "          evaluation_strategy = \"epoch\", #To calculate metrics per epoch\n",
    "          save_strategy = \"epoch\"\n",
    "  )\n",
    "  #get class weight\n",
    "  class_weights = get_class_weights(train_df)\n",
    "  CustomTrainer = create_custom_trainer(class_weights)\n",
    "\n",
    "  if train_mode == \"custom\":\n",
    "    trainer = CustomTrainer(model = model, args =training_args, train_dataset=train_dataset, eval_dataset=dev_dataset)\n",
    "  else:\n",
    "    trainer = Trainer(model = model, args =training_args, train_dataset=train_dataset, eval_dataset=dev_dataset)\n",
    "  trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd4cbc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataframe, selected_material_type=None):\n",
    "  #convert the dataframe labels accordingly by the material type\n",
    "  # original preprocess from Sarah Song\n",
    "  #if selected_material_type!=\"None\":\n",
    "  #  new_df = dataframe.copy()\n",
    "  #  for _, row in new_df.iterrows():\n",
    "  #    if row['iSampleMaterial'].split(\"_\")[0] == selected_material_type:\n",
    "  #      continue #leave the label\n",
    "  #    else:\n",
    "  #     row['iSampleMaterial']=\"None\" #set none as label\n",
    "  #else:\n",
    "  new_df = dataframe.copy()   \n",
    "  \n",
    "  #convert labels into integers\n",
    "  le.fit(new_df.iSampleMaterial)\n",
    "  print(\" number of labels: \", len(le.classes_))\n",
    "  new_df['iSampleMaterial'] = le.transform(new_df.iSampleMaterial)\n",
    "  \n",
    "  #split data to training df, dev df, test df\n",
    "  sample_size = 10000\n",
    "  #fraction=sample_size/len(new_df)  # get about 500 samples\n",
    "  #sel_len = sample_size\n",
    "  train_df, dev_df, test_df =  np.split(new_df.sample(n=sample_size, random_state=42),[int(.6*sample_size), int(.8*sample_size)])\n",
    "\n",
    "  train_df.to_csv('output/train_df.csv')\n",
    "  dev_df.to_csv('output/dev_df.csv')\n",
    "  test_df.to_csv('output/test_df.csv')\n",
    " \n",
    "  return train_df, dev_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "427fa1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_2(dataframe):\n",
    "    \n",
    "  le.fit(dataframe.iSampleMaterial)\n",
    "  print(\" number of labels: \", len(le.classes_))\n",
    "  print(\"label encoder classes:\",le.classes_)\n",
    "  rockint = le.transform([\"mat:rock\"])[0]\n",
    "  #convert the dataframe labels accordingly by the material type\n",
    "  # want to sample the different material types according to their frequency\n",
    "  # generate separate dataframes for mineral, rock, sediment, rockorsediment, and soil\n",
    "\n",
    "  # fraction of total for each class\n",
    "  #min_n = .255\n",
    "  #rock_n = .435\n",
    "  #sed_n = .125\n",
    "  #rocksed_n = .065\n",
    "  #soil_n = .12\n",
    "\n",
    "#try different distribution\n",
    "  min_n = .205\n",
    "  rock_n = .235\n",
    "  sed_n = .225\n",
    "  rocksed_n = .215\n",
    "  soil_n = .12\n",
    "  # total sample \n",
    "  sample_size = 1000\n",
    "  #rand_state = int(42)\n",
    "  rand_state = int(19)\n",
    "  \n",
    "  \n",
    "  #split data to training df, val df, test df\n",
    "  min_df = dataframe[dataframe[\"iSampleMaterial\"]==\"mat:mineral\"].copy()   #flattened label version \n",
    "    # build the data frames\n",
    "  print(\"min_df rowcount: \", len(min_df.index))\n",
    "  this_n = int(round(min_n * sample_size, 0))  # weights='weight', axis=0,\n",
    "  train_df_min, dev_df_min, test_df_min =  np.split(min_df.sample(n=this_n,   random_state=rand_state),[int(.6*this_n), int(.8*this_n)])\n",
    "  print(\"finished min dataframe. this_n:\", this_n, \" split at:\",int(.6*this_n), int(.8*this_n))\n",
    "\n",
    "  rock_df = dataframe[dataframe[\"iSampleMaterial\"]==\"mat:rock\"].copy()   #flattened label version \n",
    "    # build the data frames\n",
    "  this_n = int(round(rock_n * sample_size, 0))  # weights='weight', axis=0,\n",
    "  train_df_rock, dev_df_rock, test_df_rock =  np.split(rock_df.sample(n=this_n,   random_state=rand_state),[int(.6*this_n), int(.8*this_n)])\n",
    "  print(\"finished rock dataframe. this_n:\", this_n, \" split at:\",int(.6*this_n), int(.8*this_n))\n",
    "\n",
    "    \n",
    "  sed_df = dataframe[dataframe[\"iSampleMaterial\"]==\"mat:sediment\"].copy()   #flattened label version \n",
    "    # build the data frames\n",
    "  this_n = int(round(sed_n * sample_size, 0))  # weights='weight', axis=0,\n",
    "  train_df_sed, dev_df_sed, test_df_sed =  np.split(sed_df.sample(n=this_n,   random_state=rand_state),[int(.6*this_n), int(.8*this_n)])\n",
    "  print(\"finished sed dataframe. this_n:\", this_n, \" split at:\",int(.6*this_n), int(.8*this_n))\n",
    "\n",
    "\n",
    "  rocksed_df = dataframe[dataframe[\"iSampleMaterial\"]==\"mat:rockorsediment\"].copy()   #flattened label version \n",
    "    # build the data frames\n",
    "  this_n = int(round(rocksed_n * sample_size, 0)) # weights='weight', axis=0,\n",
    "  train_df_rocksed, dev_df_rocksed, test_df_rocksed =  np.split(rocksed_df.sample(n=this_n,   random_state=rand_state),[int(.6*this_n), int(.8*this_n)])\n",
    "  print(\"finished rocksed dataframe. this_n:\", this_n, \" split at:\",int(.6*this_n), int(.8*this_n))\n",
    "\n",
    "\n",
    "  soil_df = dataframe[dataframe[\"iSampleMaterial\"]==\"mat:soil\"].copy()   #flattened label version \n",
    "    # build the data frames\n",
    "  this_n = int(round(soil_n * sample_size, 0)) # weights='weight', axis=0,\n",
    "  train_df_soil, dev_df_soil, test_df_soil =  np.split(soil_df.sample(n=this_n,   random_state=rand_state),[int(.6*this_n), int(.8*this_n)])\n",
    "  print(\"finished soil dataframe. this_n:\", this_n, \" split at:\",int(.6*this_n), int(.8*this_n))\n",
    "\n",
    "    \n",
    "  #train_df_soil.to_csv('output/train_df_soil.csv')\n",
    "  #dev_df_soil.to_csv('output/dev_df_soil.csv')\n",
    "  #test_df_soil.to_csv('output/test_df_soil.csv')\n",
    "    \n",
    "    \n",
    "#intention is for final dataset for training to have sample_size records, distributed over the 5 classes based on the abundance of the class\n",
    "#  and weighted according to the frequency distribution for the 154 IGSN registrants. Based on \n",
    "#  assumption that a given registrant will be documenting similar samples with similar conventions\n",
    "    \n",
    "# merge the training dataframes\n",
    "  theframes = [train_df_min, train_df_rock, train_df_sed,train_df_rocksed,train_df_soil]\n",
    "  train_df = pd.concat(theframes)\n",
    "  train_df.sort_values(by='igsn', inplace=True )\n",
    "  #convert labels into integers\n",
    "  train_df['iSampleMaterial'] = le.transform(train_df.iSampleMaterial)\n",
    "  \n",
    "# merge the dev dataframes\n",
    "  theframes = [dev_df_min, dev_df_rock, dev_df_sed,dev_df_rocksed,dev_df_soil]\n",
    "  dev_df = pd.concat(theframes)\n",
    "  dev_df.sort_values(by='igsn', inplace=True )\n",
    "  #convert labels into integers\n",
    "  dev_df['iSampleMaterial'] = le.transform(dev_df.iSampleMaterial)\n",
    "\n",
    "# merge the training dataframes\n",
    "  theframes = [test_df_min, test_df_rock, test_df_sed,test_df_rocksed,test_df_soil]\n",
    "  test_df = pd.concat(theframes) \n",
    "  test_df.sort_values(by='igsn', inplace=True )\n",
    "  #convert labels into integers\n",
    "  test_df['iSampleMaterial'] = le.transform(test_df.iSampleMaterial)\n",
    "\n",
    "  train_df.to_csv('output/train_df.csv')\n",
    "  dev_df.to_csv('output/dev_df.csv')\n",
    "  test_df.to_csv('output/test_df.csv')\n",
    "    \n",
    "  return train_df, dev_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87228d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # use dictionary of classes \n",
    "# SMR 2023-08-21\n",
    "\n",
    "def preprocess_3(dataframe):\n",
    "  # classname : samplesize \n",
    "  classdict = {\n",
    "    \"mat:rock\" : 500,\n",
    "    \"mat:mineral\" : 500,\n",
    "    \"mat:organicmaterial\" : 500,\n",
    "    \"mat:sediment\" : 500,\n",
    "    \"mat:soil\" : 500,\n",
    "    \"mat:liquidwater\" : 500,\n",
    "    \"mat:material\" : 400,\n",
    "    \"mat:rockorsediment\" : 400,\n",
    "    \"mat:mixedsoilsedimentrock\" : 300,\n",
    "    \"mat:biogenicnonorganicmaterial\" : 300,\n",
    "    \"mat:otheranthropogenicmaterial\" : 200,\n",
    "    \"mat:particulate\" : 200,\n",
    "    \"xxx\" : 150,\n",
    "    \"mat:gas\" : 200,\n",
    "    \"mat:anthropogenicmetal\" : 50\n",
    "  }\n",
    "\n",
    "  classcol = \"iSampleMaterial\"\n",
    "  rand_state = int(19)\n",
    "  samplesize = int(0)\n",
    "  classname = \"\"\n",
    "    \n",
    "  #empty data frames to accumulate results  \n",
    "  work_df = pd.DataFrame()\n",
    "  train_df  = pd.DataFrame() \n",
    "  dev_df = pd.DataFrame()\n",
    "  test_df = pd.DataFrame()\n",
    "\n",
    "  le.fit(dataframe.iSampleMaterial)\n",
    "  print(\" number of labels: \", len(le.classes_))\n",
    "  print(\"label encoder classes:\",le.classes_)\n",
    "  print(\"transform classes:\", le.fit_transform(le.classes_))\n",
    "    \n",
    "  for classname, samplesize in classdict.items() :\n",
    "  #split data to training df, val df, test df\n",
    "    #print(\"class:\", classname, \"  samplesize:\",samplesize)\n",
    "    work_df = dataframe[dataframe[classcol]==classname].copy()   #flattened label version \n",
    "        # build the data frames\n",
    "    #print(classname, \" rowcount: \", len(work_df.index))\n",
    "    train_df_work, dev_df_work, test_df_work =  np.split(work_df.sample(n=samplesize,   random_state=rand_state),[int(.6*samplesize), int(.8*samplesize)])\n",
    "    print(\"finished \",classname,\" dataframe. samplesize:\", samplesize, \" split at:\",int(.6*samplesize), int(.8*samplesize))\n",
    "     \n",
    "    # merge into the output dataframes\n",
    "    train_df = pd.concat([train_df_work, train_df]) \n",
    "    test_df = pd.concat([test_df_work, test_df]) \n",
    "    dev_df = pd.concat([dev_df_work, dev_df]) \n",
    "  \n",
    "  \n",
    "  #sort by igsn, convert labels to integers  \n",
    "  train_df.sort_values(by='igsn', inplace=True )\n",
    "  train_df['iSampleMaterial'] = le.transform(train_df.iSampleMaterial)\n",
    "  test_df.sort_values(by='igsn', inplace=True )\n",
    "  test_df['iSampleMaterial'] = le.transform(test_df.iSampleMaterial)\n",
    "  dev_df.sort_values(by='igsn', inplace=True )\n",
    "  dev_df['iSampleMaterial'] = le.transform(dev_df.iSampleMaterial)\n",
    "\n",
    "  #write dataframes for reference\n",
    "  train_df.to_csv('output/train_df.csv')\n",
    "  dev_df.to_csv('output/dev_df.csv')\n",
    "  test_df.to_csv('output/test_df.csv')\n",
    "    \n",
    "  return train_df, dev_df, test_df\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bc1072",
   "metadata": {},
   "source": [
    " # Notes\n",
    " \n",
    " epochs 4, batch 20, lr_rat .007 worked best yet, with 500 samples; lowest loses at 3 epochs\n",
    " \n",
    " try adding axis = 0 in pandas sampling, I can't tell if its using weights. different results-- sed and rocksed are bad,\n",
    "  others much better. Run again to see if the same... Get different results. The pandas sample is different, and that \n",
    " \n",
    " impacts the results. 3 of 5 classes identified prttey well. Try raising sample to 1000.  Got matches on 4 classes, \n",
    " good only on mineral, rock, and soil.\n",
    " \n",
    " try sampling w/o weights\n",
    "  !! worked much better!!\n",
    "\n",
    " 2023-08-15\n",
    " \n",
    " try 10 epochs (n-1000, .007, batch 20) to see if get any convergence,no convergence, \n",
    "        but recall and precision not bad\n",
    " \n",
    " try 3 epochs, rest same. Seemed to work about as well\n",
    " \n",
    " try 1 epoch-- seemed to work pretty much the same\n",
    " \n",
    " try 3 epoch, 40 batch, 5000 samples-- complete fail! \n",
    " \n",
    " try 3 epoch, 100 batch, 5000 samples\n",
    " \n",
    " 1 epoch 100 batch 5000, SS's preprocess. Doesn't get any rockSed, otherwise goot\n",
    " \n",
    " 1 epoch 100 batch, 10000 sample, SS preprocess. didn't get any rockSed or soil...\n",
    " \n",
    " 4 epoch 100 batch,  10000 sample, SMR preprocess.  TErrible. only got rock\n",
    " \n",
    " 4 epoch 100 batch,  10000 sample, ss preprocess.  Terrible. only got rock\n",
    " \n",
    " 3 epoch, 20 batch 10000 sample, ss preprocess rate.01 Bust only got rock\n",
    " \n",
    " 3 epoch, 20 batch 10000 sample, smr preprocess rate.01 Bust only got rock, but some  convergences\n",
    " \n",
    " 4 epoch,batch20,.007,1000 samples, change fractions in training data to favor sed,rocksed. \n",
    "    # good matches except rocksed, but some hits theretoo. Got convergence after the first epoch\n",
    " \n",
    " try more epochs\n",
    " \n",
    " 8 epoch,batch20,.007,1000 samples, change fractions in training data to favor sed,rocksed. \n",
    "    # good result rand 42, 23\n",
    "\n",
    "results vary slightly with different rand\n",
    " \n",
    " raise n to 2000, rand=73, bad result \n",
    " \n",
    " try 4 epoch rand= 73\n",
    " \n",
    " 2023-08-21\n",
    " implement new preprocess that builds training data by selecting the classes to sample and number of samples using a dictionary.  raise training text length from 80 to 100.  Training text from SESAR has been culled to remove long    sentences about sample locations, mostly from Alan Mansur, NMNH\n",
    "  total sample is about 2000.\n",
    "  \n",
    "Results are useless.  Go back to 80 char length, and up the learning sample size, reduce epochs to 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97f3e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Required parameters\n",
    "nb_epochs = int(4)  #was 2, then 3, \n",
    "batch_size = int(20) #was 10, then 20, tried 30. \n",
    "lr_rate = float(0.007) #was.01\n",
    "\n",
    "# material_type = str('')\n",
    "\n",
    "#train_mode = str('FALSE')\n",
    "train_mode = str('custom')\n",
    " #  Whether we account for class imbalance during training by using a custom trainer \n",
    "    # (custom) or not (none)                  \n",
    "output_dir =str('output')\n",
    " #Output directory where the model checkpoint will be saved\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de78509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_csv(\"iSamplesMaterialTrainingSmall.csv\")\n",
    "#df = pd.read_csv(\"SESARTrainingiSamKeywords.csv\", usecols=['igsn', 'traintext'],dtype={'igsn':str,'traintext':str})\n",
    "#df = pd.read_csv(\"MaterialTypeData2023-08-07.csv\") # only has rock, sediment, rocksed, soil, mineral\n",
    "\n",
    "classcol = \"iSampleMaterial\"\n",
    "traintextcol = 'traintext'\n",
    "df = pd.read_csv(\"SESARTrainingiSamKeywords.csv\", usecols=['igsn', classcol, traintextcol],dtype={'igsn':str, classcol:str, traintextcol:str})\n",
    "\n",
    "df = df.fillna(\"\")\n",
    "#remove rows that do not have a class name or training text\n",
    "df = df[df[classcol]!=\"\"]\n",
    "df = df[df[traintextcol]!=\"\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249e387a",
   "metadata": {},
   "source": [
    "#count tokens-- \n",
    "####################### SLOW-- this scans all records.\n",
    "rowcount = 1\n",
    "ratiosum = 0.0\n",
    "maxratio = 0.0\n",
    "for _, row in df.iterrows():\n",
    "    sentence = row[\"text\"]\n",
    "    tokens = tokenizer.tokenize(sentence)\n",
    "    token_count = len(tokens)\n",
    "    senlen = len(sentence)\n",
    "    ratio = token_count/senlen\n",
    "    if ratio > maxratio:\n",
    "        maxratio = ratio\n",
    "    \n",
    "    #print(\"Original sentence:\", sentence)\n",
    "    #print(\"Sentence len:\", senlen, \"; Number of tokens:\", token_count, \"; ratio:\", ratio)\n",
    "    #print(\"Number of tokens:\", token_count)\n",
    "    \n",
    "    rowcount =rowcount + 1\n",
    "    ratiosum = ratiosum + ratio\n",
    "    #print(\"ratio:\", ratio)\n",
    "    \n",
    "avrage = ratiosum/rowcount\n",
    "print(\"Average ratio:\", avrage, \"; Max ratio:\", maxratio)\n",
    "print(\"row count: \", rowcount)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6ab962d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " number of labels:  15\n"
     ]
    }
   ],
   "source": [
    "#load tokenizer\n",
    "# train(material_type, df, tokenizer, batch_size,lr_rate, nb_epochs, train_mode, output_dir)\n",
    "\n",
    "# insert train function in line here for debugging...\n",
    "train_df, dev_df, test_df = preprocess(df)  #original function from Sarah Song\n",
    "#train_df, dev_df, test_df = preprocess_2(df)  #steves update, only rock, mineral, rocksed, soil, sediment\n",
    "#train_df, dev_df, test_df = preprocess_3(df)  #dictionary to set sample size for each class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dae570a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch tensor dataframe columns: ['igsn' 'traintext' 'iSampleMaterial']\n",
      "torch tensor dataframe columns: ['igsn' 'traintext' 'iSampleMaterial']\n",
      "torch tensor dataframe columns: ['igsn' 'traintext' 'iSampleMaterial']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#print(\"train_df columns:\", train_df.columns.values)\n",
    "#print(\"train_df:\", train_df.describe)\n",
    "#train_df['iSampleMaterial'].values\n",
    "\n",
    "train_dataset = create_dataset(train_df, tokenizer)\n",
    "dev_dataset = create_dataset(dev_df,tokenizer)\n",
    "test_dataset = create_dataset(test_df,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74223cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(le.classes_))\n",
    "#print(\"transform classes:\", le.fit_transform([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9ece0d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = BertForSequenceClassification.from_pretrained(\"allenai/scibert_scivocab_uncased\", num_labels = len(le.classes_), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63818b0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "desc = model.to(device)\n",
    "training_args = TrainingArguments(\n",
    "          no_cuda = True,\n",
    "          output_dir= output_dir,     # output directory\n",
    "          num_train_epochs=nb_epochs,              # total number of training epochs\n",
    "          per_device_train_batch_size=batch_size,  # batch size per device during training\n",
    "          per_device_eval_batch_size=batch_size,   # batch size for evaluation\n",
    "          learning_rate = lr_rate,\n",
    "          warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "          weight_decay=0.01, \n",
    "          load_best_model_at_end=True,            \n",
    "          logging_dir=output_dir,            # directory for storing logs\n",
    "          logging_steps=10,\n",
    "          evaluation_strategy = \"epoch\", #To calculate metrics per epoch\n",
    "          save_strategy = \"epoch\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "906eca53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np unique labels for weights: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14]\n",
      "le class labels:  ['mat:anthropogenicmetal' 'mat:biogenicnonorganicmaterial' 'mat:gas'\n",
      " 'mat:liquidwater' 'mat:material' 'mat:mineral'\n",
      " 'mat:mixedsoilsedimentrock' 'mat:organicmaterial'\n",
      " 'mat:otheranthropogenicmaterial' 'mat:particulate' 'mat:rock'\n",
      " 'mat:rockorsediment' 'mat:sediment' 'mat:soil' 'xxx']\n",
      "labels.numpy: <built-in method numpy of Tensor object at 0x00000228C6640EF0>\n"
     ]
    }
   ],
   "source": [
    "class_weights = get_class_weights(df)\n",
    "CustomTrainer = create_custom_trainer(class_weights)\n",
    "if train_mode == \"custom\":\n",
    "\n",
    "    trainer = CustomTrainer(model = model, args =training_args, train_dataset=train_dataset, eval_dataset=dev_dataset)\n",
    "else:\n",
    "    trainer = Trainer(model = model, args =training_args, train_dataset=train_dataset, eval_dataset=dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3e6a236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset.labels\n",
    "#train_dataset.encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06516275",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\mlclassification\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1200/1200 4:09:08, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.325100</td>\n",
       "      <td>4.182779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.153400</td>\n",
       "      <td>6.508500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.923300</td>\n",
       "      <td>3.804018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.519500</td>\n",
       "      <td>2.631948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in control.should_save. metrics: {'eval_loss': 4.182779312133789}\n",
      "in control.should_save. metrics: {'eval_loss': 6.508499622344971}\n",
      "in control.should_save. metrics: {'eval_loss': 3.804018259048462}\n",
      "in control.should_save. metrics: {'eval_loss': 2.631948471069336}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1200, training_loss=3.7530531819661457, metrics={'train_runtime': 14961.581, 'train_samples_per_second': 1.604, 'train_steps_per_second': 0.08, 'total_flos': 1233477028800000.0, 'train_loss': 3.7530531819661457, 'epoch': 4.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eff6946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #get class weight.  Only need if using customTrainer\n",
    "#class_weights = get_class_weights(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "54410f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cpu\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f7ef05bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #conduct evaluation \n",
    "  keys = []\n",
    "  precision = []\n",
    "  recall = []\n",
    "  f1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d3178121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5,  7, 10,  ...,  5, 10,  5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\mlclassification\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\mlclassification\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Anaconda\\envs\\mlclassification\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1334: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "logits = trainer.predict(test_dataset)[0] #get the logits \n",
    "\n",
    "test_pred = np.argmax(logits,axis=-1)\n",
    "y_test= torch.tensor(test_df['iSampleMaterial'].values.tolist())\n",
    "\n",
    "print (y_test)\n",
    "\n",
    "res = classification_report(y_test,test_pred,output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94c1a1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(logits)\n",
    "#print(test_dataset.__getitem__(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ecd43f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(test_dataset.encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd66455d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mat:biogenicnonorganicmaterial \t\t\t 0.00 \t 0.00 \t 0.00\n",
      "mat:gas \t\t\t 0.00 \t 0.00 \t 0.00\n",
      "mat:liquidwater \t\t\t 0.00 \t 0.00 \t 0.00\n",
      "mat:material \t\t\t 0.00 \t 0.00 \t 0.00\n",
      "mat:mineral \t\t\t 0.00 \t 0.00 \t 0.00\n",
      "mat:mixedsoilsedimentrock \t\t\t 0.00 \t 0.00 \t 0.00\n",
      "mat:organicmaterial \t\t\t 0.13 \t 1.00 \t 0.23\n",
      "mat:otheranthropogenicmaterial \t\t\t 0.00 \t 0.00 \t 0.00\n",
      "mat:particulate \t\t\t 0.00 \t 0.00 \t 0.00\n",
      "mat:rock \t\t\t 0.00 \t 0.00 \t 0.00\n",
      "mat:rockorsediment \t\t\t 0.00 \t 0.00 \t 0.00\n",
      "mat:sediment \t\t\t 0.00 \t 0.00 \t 0.00\n",
      "mat:soil \t\t\t 0.00 \t 0.00 \t 0.00\n"
     ]
    }
   ],
   "source": [
    "for key, score in res.items():\n",
    "  if key.isdigit():\n",
    "    keys.append((le.inverse_transform([int(key)])[0]))\n",
    "    precision.append(round(score['precision'],2))\n",
    "    recall.append(round(score['recall'],2))\n",
    "    f1.append(round(score['f1-score'],2))\n",
    "    print(\"%s \\t\\t\\t %0.2f \\t %0.2f \\t %0.2f\"% (le.inverse_transform([int(key)])[0],score['precision'], score['recall'], score['f1-score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e8cba5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro average:  0.017578524221571166\n"
     ]
    }
   ],
   "source": [
    "#write the results to excel and save\n",
    "result_df = pd.DataFrame(data=zip(keys,precision,recall,f1), columns=['label','precision','recall','f1'])\n",
    "result_output_dir =\"output/sesar_result.xlsx\"\n",
    "result_df.to_excel(result_output_dir)\n",
    "print(\"Macro average: \",f1_score(y_test,test_pred,average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f998ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
