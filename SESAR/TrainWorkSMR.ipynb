{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d63c2139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer,BertForSequenceClassification,Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn import preprocessing\n",
    "import torch.nn as nn\n",
    "from sklearn.utils import compute_class_weight\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ed4a71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_MODE\"]=\"disabled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98379953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "10698e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "# use this to map categories to integers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfaff96",
   "metadata": {},
   "source": [
    "functions and class definitions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba50c63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassDataset(Dataset):\n",
    "\n",
    "    def __init__(self, encodings, labels):\n",
    "      self.encodings = encodings\n",
    "      self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54c732a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataframe, selected_material_type=None):\n",
    "  #convert the dataframe labels accordingly by the material type\n",
    "  if selected_material_type!=\"None\":\n",
    "    new_df = dataframe.copy()\n",
    "    for _, row in new_df.iterrows():\n",
    "      if row['description_material'].split(\"_\")[0] == selected_material_type:\n",
    "        continue #leave the label\n",
    "      else:\n",
    "        row['description_material']=\"None\" #set none as label\n",
    "  else:\n",
    "    new_df = dataframe.copy()   #flattened label version \n",
    "  new_df\n",
    "  #convert labels into integers\n",
    "  le.fit(new_df.description_material)\n",
    "\n",
    "  new_df['description_material'] = le.transform(new_df.description_material)\n",
    "  print(\" number of labels: \", len(le.classes_))\n",
    "  #split data to training df, val df, test df\n",
    "  train_df, dev_df, test_df =  np.split(new_df.sample(frac=1, random_state=42),[int(.6*len(new_df)), int(.8*len(new_df))])\n",
    " \n",
    "  return train_df, dev_df, test_df\n",
    "\n",
    "\n",
    "def create_dataset(dataframe, tokenizer):\n",
    "  MAX_LENGTH = 64\n",
    "  inputs = {\n",
    "          \"input_ids\":[],\n",
    "          \"attention_mask\":[]\n",
    "        }\n",
    "  features_columns =[x for x in dataframe.columns.values if x != 'description_material' and x.startswith(\"description\")]\n",
    "  def create_concatenated_text(dataframe):\n",
    "    \"\"\"combine the columns text to create a single sentence\"\"\"\n",
    "    sents= [] #text that is a concatenation of all columns\n",
    "    for _, row in dataframe.iterrows():\n",
    "      combined = \"\"\n",
    "      for col in features_columns:\n",
    "        row_value = row[col]\n",
    "        if row_value!=\"\" and type(row_value)==str:\n",
    "          combined+= row_value +\" , \"\n",
    "      sents.append(combined)\n",
    "    return sents\n",
    "  sents = create_concatenated_text(dataframe)\n",
    "  for sent in sents:\n",
    "    tokenized_input = tokenizer(sent,max_length=MAX_LENGTH, padding='max_length', truncation=True)\n",
    "    inputs[\"input_ids\"].append(torch.tensor(tokenized_input[\"input_ids\"]))\n",
    "    inputs[\"attention_mask\"].append(torch.tensor(tokenized_input[\"attention_mask\"]))\n",
    "\n",
    "  labels = torch.tensor(dataframe['description_material'].values.tolist())\n",
    "\n",
    "  return MulticlassDataset(inputs,labels)\n",
    "\n",
    "def get_class_weights(dataframe):\n",
    "  \"\"\"computes the class weight and returns a list to account for class imbalance \"\"\"\n",
    "  labels = torch.tensor(dataframe['description_material'].values.tolist())\n",
    "  class_weights=compute_class_weight( class_weight ='balanced',classes = np.unique(labels),y = labels.numpy())\n",
    "\n",
    "  total_class_weights =torch.tensor(class_weights,dtype=torch.float).to(device)\n",
    "  return total_class_weights\n",
    "\n",
    "def create_custom_trainer(class_weights):\n",
    "  \"\"\"creates custom trainer that accounts for class imbalance\"\"\"\n",
    "  class CustomTrainer(Trainer):\n",
    "      def compute_loss(self, model, inputs, return_outputs=False):\n",
    "          labels = inputs.get(\"labels\")\n",
    "          # forward pass\n",
    "          outputs = model(**inputs)\n",
    "          logits = outputs.get(\"logits\")\n",
    "          # compute custom loss \n",
    "          loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
    "          loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "          return (loss, outputs) if return_outputs else loss\n",
    "  return CustomTrainer\n",
    "\n",
    "def train(selected_type, dataframe, tokenizer, batch_size, learning_rate, epochs,train_mode, output_dir):\n",
    "\n",
    "  train_df, dev_df, test_df = preprocess(dataframe,selected_type)\n",
    "  train_dataset = create_dataset(train_df, tokenizer)\n",
    "  dev_dataset = create_dataset(dev_df,tokenizer)\n",
    "  test_dataset = create_dataset(test_df,tokenizer)\n",
    "\n",
    "  #load model\n",
    "  model = BertForSequenceClassification.from_pretrained(\"allenai/scibert_scivocab_uncased\", num_labels = len(le.classes_), )\n",
    "\n",
    "  # Tell pytorch to run this model on the GPU.\n",
    "  desc = model.cuda()\n",
    "\n",
    "  training_args = TrainingArguments(\n",
    "          output_dir= output_dir,     # output directory\n",
    "          num_train_epochs=epochs,              # total number of training epochs\n",
    "          per_device_train_batch_size=batch_size,  # batch size per device during training\n",
    "          per_device_eval_batch_size=batch_size,   # batch size for evaluation\n",
    "          learning_rate = learning_rate,\n",
    "          warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "          weight_decay=0.01, \n",
    "          load_best_model_at_end=True,            \n",
    "          logging_dir=output_dir,            # directory for storing logs\n",
    "          logging_steps=10,\n",
    "          evaluation_strategy = \"epoch\", #To calculate metrics per epoch\n",
    "          save_strategy = \"epoch\"\n",
    "  )\n",
    "  #get class weight\n",
    "  class_weights = get_class_weights(train_df)\n",
    "  CustomTrainer = create_custom_trainer(class_weights)\n",
    "\n",
    "  if train_mode == \"custom\":\n",
    "    trainer = CustomTrainer(model = model, args =training_args, train_dataset=train_dataset, eval_dataset=dev_dataset)\n",
    "  else:\n",
    "    trainer = Trainer(model = model, args =training_args, train_dataset=train_dataset, eval_dataset=dev_dataset)\n",
    "  trainer.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97f3e165",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Required parameters\n",
    "nb_epochs = int(2)\n",
    "\n",
    "batch_size = int(10)\n",
    "lr_rate = float(0.01)\n",
    "\n",
    "material_type = str('')\n",
    "\n",
    "train_mode = str('FALSE'),\n",
    " #  Whether we account for class imbalance during training by using a custom trainer \n",
    "    # (custom) or not (none)                  \n",
    "output_dir =str('output')\n",
    " #Output directory where the model checkpoint will be saved\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de78509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"iSamplesMaterialTrainingSmall.csv\")\n",
    "df = df.fillna(\"\")\n",
    "    #remove rows that do not have a material type\n",
    "df = df[df[\"description_material\"]!=\"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64ced250",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #load tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained('allenai/scibert_scivocab_uncased', do_lower_case=True, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6ab962d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " number of labels:  3\n"
     ]
    }
   ],
   "source": [
    "#load tokenizer\n",
    "# train(material_type, df, tokenizer, batch_size,lr_rate, nb_epochs, train_mode, output_dir)\n",
    "\n",
    "# insert train function in line here for debugging...\n",
    "train_df, dev_df, test_df = preprocess(df,'None')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8dae570a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(train_df, tokenizer)\n",
    "dev_dataset = create_dataset(dev_df,tokenizer)\n",
    "test_dataset = create_dataset(test_df,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9ece0d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = BertForSequenceClassification.from_pretrained(\"allenai/scibert_scivocab_uncased\", num_labels = len(le.classes_), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f70d0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Tell pytorch to run this model on the GPU.\n",
    "    # have to use the GPU check from the beginning...\n",
    "# desc = model.cuda()\n",
    "# Using the `Trainer` with `PyTorch` requires `accelerate`: Run `pip install --upgrade accelerate`\n",
    "# is this because we don't have a GPU?\n",
    "desc = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63818b0c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "          output_dir= output_dir,     # output directory\n",
    "          num_train_epochs=nb_epochs,              # total number of training epochs\n",
    "          per_device_train_batch_size=batch_size,  # batch size per device during training\n",
    "          per_device_eval_batch_size=batch_size,   # batch size for evaluation\n",
    "          learning_rate = lr_rate,\n",
    "          warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "          weight_decay=0.01, \n",
    "          load_best_model_at_end=True,            \n",
    "          logging_dir=output_dir,            # directory for storing logs\n",
    "          logging_steps=10,\n",
    "          evaluation_strategy = \"epoch\", #To calculate metrics per epoch\n",
    "          save_strategy = \"epoch\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eff6946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "  #get class weight\n",
    "class_weights = get_class_weights(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "906eca53",
   "metadata": {},
   "outputs": [],
   "source": [
    "CustomTrainer = create_custom_trainer(class_weights)\n",
    "if train_mode == \"custom\":\n",
    "    trainer = CustomTrainer(model = model, args =training_args, train_dataset=train_dataset, eval_dataset=dev_dataset)\n",
    "else:\n",
    "    trainer = Trainer(model = model, args =training_args, train_dataset=train_dataset, eval_dataset=dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06516275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\sampleclass\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14' max='14' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14/14 31:33, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.335818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.812100</td>\n",
       "      <td>0.248678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=14, training_loss=0.7254903997693744, metrics={'train_runtime': 2034.3078, 'train_samples_per_second': 0.062, 'train_steps_per_second': 0.007, 'total_flos': 4144036329216.0, 'train_loss': 0.7254903997693744, 'epoch': 2.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54410f67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
